{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9zgtWFoM2ECW",
        "outputId": "fe5d5188-dd11-46ce-9caf-51bfa898dea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "         Date  Temp\n",
            "0  1981-01-01  20.7\n",
            "1  1981-01-02  17.9\n",
            "2  1981-01-03  18.8\n",
            "3  1981-01-04  14.6\n",
            "4  1981-01-05  15.8\n",
            "\n",
            "Total samples: 3620\n",
            "Training samples: 2896\n",
            "Testing samples: 724\n",
            "\n",
            "Total hyperparameter combinations: 128\n",
            "\n",
            "Training model 1/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0678, MSE: 0.0075\n",
            "\n",
            "Training model 2/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0663, MSE: 0.0071\n",
            "\n",
            "Training model 3/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0072, MAE: 0.0669, MSE: 0.0072\n",
            "\n",
            "Training model 4/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0665, MSE: 0.0072\n",
            "\n",
            "Training model 5/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0681, MSE: 0.0075\n",
            "\n",
            "Training model 6/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 7/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0070, MAE: 0.0660, MSE: 0.0070\n",
            "\n",
            "Training model 8/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0070, MAE: 0.0660, MSE: 0.0070\n",
            "\n",
            "Training model 9/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0680, MSE: 0.0075\n",
            "\n",
            "Training model 10/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0664, MSE: 0.0072\n",
            "\n",
            "Training model 11/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0663, MSE: 0.0071\n",
            "\n",
            "Training model 12/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0667, MSE: 0.0071\n",
            "\n",
            "Training model 13/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0098, MAE: 0.0767, MSE: 0.0098\n",
            "\n",
            "Training model 14/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0091, MAE: 0.0739, MSE: 0.0091\n",
            "\n",
            "Training model 15/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0663, MSE: 0.0071\n",
            "\n",
            "Training model 16/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0665, MSE: 0.0071\n",
            "\n",
            "Training model 17/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0076, MAE: 0.0681, MSE: 0.0076\n",
            "\n",
            "Training model 18/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0666, MSE: 0.0072\n",
            "\n",
            "Training model 19/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0662, MSE: 0.0071\n",
            "\n",
            "Training model 20/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0669, MSE: 0.0072\n",
            "\n",
            "Training model 21/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0677, MSE: 0.0075\n",
            "\n",
            "Training model 22/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0077, MAE: 0.0684, MSE: 0.0077\n",
            "\n",
            "Training model 23/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0680, MSE: 0.0075\n",
            "\n",
            "Training model 24/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0661, MSE: 0.0071\n",
            "\n",
            "Training model 25/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0076, MAE: 0.0679, MSE: 0.0076\n",
            "\n",
            "Training model 26/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0078, MAE: 0.0699, MSE: 0.0078\n",
            "\n",
            "Training model 27/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0674, MSE: 0.0073\n",
            "\n",
            "Training model 28/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 29/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0087, MAE: 0.0722, MSE: 0.0087\n",
            "\n",
            "Training model 30/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0681, MSE: 0.0076\n",
            "\n",
            "Training model 31/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0076, MAE: 0.0689, MSE: 0.0076\n",
            "\n",
            "Training model 32/128\n",
            "Activation: tanh, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0660, MSE: 0.0071\n",
            "\n",
            "Training model 33/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0675, MSE: 0.0073\n",
            "\n",
            "Training model 34/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0662, MSE: 0.0071\n",
            "\n",
            "Training model 35/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0072, MAE: 0.0668, MSE: 0.0072\n",
            "\n",
            "Training model 36/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0070, MAE: 0.0660, MSE: 0.0070\n",
            "\n",
            "Training model 37/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0074, MAE: 0.0674, MSE: 0.0074\n",
            "\n",
            "Training model 38/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0073, MAE: 0.0672, MSE: 0.0073\n",
            "\n",
            "Training model 39/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0664, MSE: 0.0071\n",
            "\n",
            "Training model 40/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0688, MSE: 0.0076\n",
            "\n",
            "Training model 41/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0072, MAE: 0.0666, MSE: 0.0072\n",
            "\n",
            "Training model 42/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0663, MSE: 0.0071\n",
            "\n",
            "Training model 43/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 44/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0664, MSE: 0.0071\n",
            "\n",
            "Training model 45/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0079, MAE: 0.0696, MSE: 0.0079\n",
            "\n",
            "Training model 46/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0073, MAE: 0.0673, MSE: 0.0073\n",
            "\n",
            "Training model 47/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0070, MAE: 0.0660, MSE: 0.0070\n",
            "\n",
            "Training model 48/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0075, MAE: 0.0678, MSE: 0.0075\n",
            "\n",
            "Training model 49/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0664, MSE: 0.0071\n",
            "\n",
            "Training model 50/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0664, MSE: 0.0071\n",
            "\n",
            "Training model 51/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0669, MSE: 0.0073\n",
            "\n",
            "Training model 52/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0663, MSE: 0.0071\n",
            "\n",
            "Training model 53/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0092, MAE: 0.0759, MSE: 0.0092\n",
            "\n",
            "Training model 54/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0664, MSE: 0.0071\n",
            "\n",
            "Training model 55/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0679, MSE: 0.0075\n",
            "\n",
            "Training model 56/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0661, MSE: 0.0071\n",
            "\n",
            "Training model 57/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 58/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 59/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0672, MSE: 0.0073\n",
            "\n",
            "Training model 60/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0667, MSE: 0.0072\n",
            "\n",
            "Training model 61/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0093, MAE: 0.0748, MSE: 0.0093\n",
            "\n",
            "Training model 62/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0077, MAE: 0.0687, MSE: 0.0077\n",
            "\n",
            "Training model 63/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0672, MSE: 0.0073\n",
            "\n",
            "Training model 64/128\n",
            "Activation: tanh, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0683, MSE: 0.0076\n",
            "\n",
            "Training model 65/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0074, MAE: 0.0668, MSE: 0.0074\n",
            "\n",
            "Training model 66/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0073, MAE: 0.0665, MSE: 0.0073\n",
            "\n",
            "Training model 67/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0077, MAE: 0.0685, MSE: 0.0077\n",
            "\n",
            "Training model 68/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0073, MAE: 0.0670, MSE: 0.0073\n",
            "\n",
            "Training model 69/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0079, MAE: 0.0691, MSE: 0.0079\n",
            "\n",
            "Training model 70/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0087, MAE: 0.0725, MSE: 0.0087\n",
            "\n",
            "Training model 71/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0109, MAE: 0.0838, MSE: 0.0109\n",
            "\n",
            "Training model 72/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0665, MSE: 0.0072\n",
            "\n",
            "Training model 73/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0088, MAE: 0.0728, MSE: 0.0088\n",
            "\n",
            "Training model 74/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0677, MSE: 0.0076\n",
            "\n",
            "Training model 75/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0095, MAE: 0.0752, MSE: 0.0095\n",
            "\n",
            "Training model 76/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0078, MAE: 0.0690, MSE: 0.0078\n",
            "\n",
            "Training model 77/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0100, MAE: 0.0769, MSE: 0.0100\n",
            "\n",
            "Training model 78/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0094, MAE: 0.0750, MSE: 0.0094\n",
            "\n",
            "Training model 79/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0079, MAE: 0.0701, MSE: 0.0079\n",
            "\n",
            "Training model 80/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0080, MAE: 0.0693, MSE: 0.0080\n",
            "\n",
            "Training model 81/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0087, MAE: 0.0727, MSE: 0.0087\n",
            "\n",
            "Training model 82/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0075, MAE: 0.0672, MSE: 0.0075\n",
            "\n",
            "Training model 83/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0076, MAE: 0.0680, MSE: 0.0076\n",
            "\n",
            "Training model 84/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0077, MAE: 0.0685, MSE: 0.0077\n",
            "\n",
            "Training model 85/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0089, MAE: 0.0733, MSE: 0.0089\n",
            "\n",
            "Training model 86/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0085, MAE: 0.0718, MSE: 0.0085\n",
            "\n",
            "Training model 87/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0668, MSE: 0.0073\n",
            "\n",
            "Training model 88/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0074, MAE: 0.0677, MSE: 0.0074\n",
            "\n",
            "Training model 89/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0088, MAE: 0.0724, MSE: 0.0088\n",
            "\n",
            "Training model 90/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0078, MAE: 0.0684, MSE: 0.0078\n",
            "\n",
            "Training model 91/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0101, MAE: 0.0779, MSE: 0.0101\n",
            "\n",
            "Training model 92/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0681, MSE: 0.0076\n",
            "\n",
            "Training model 93/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0096, MAE: 0.0758, MSE: 0.0096\n",
            "\n",
            "Training model 94/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0094, MAE: 0.0748, MSE: 0.0094\n",
            "\n",
            "Training model 95/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0081, MAE: 0.0700, MSE: 0.0081\n",
            "\n",
            "Training model 96/128\n",
            "Activation: relu, Neurons: 50, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0085, MAE: 0.0717, MSE: 0.0085\n",
            "\n",
            "Training model 97/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0071, MAE: 0.0659, MSE: 0.0071\n",
            "\n",
            "Training model 98/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0665, MSE: 0.0072\n",
            "\n",
            "Training model 99/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0678, MSE: 0.0075\n",
            "\n",
            "Training model 100/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0076, MAE: 0.0679, MSE: 0.0076\n",
            "\n",
            "Training model 101/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0080, MAE: 0.0694, MSE: 0.0080\n",
            "\n",
            "Training model 102/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0075, MAE: 0.0671, MSE: 0.0075\n",
            "\n",
            "Training model 103/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0076, MAE: 0.0689, MSE: 0.0076\n",
            "\n",
            "Training model 104/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0070, MAE: 0.0657, MSE: 0.0070\n",
            "\n",
            "Training model 105/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0080, MAE: 0.0694, MSE: 0.0080\n",
            "\n",
            "Training model 106/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0072, MAE: 0.0663, MSE: 0.0072\n",
            "\n",
            "Training model 107/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0077, MAE: 0.0687, MSE: 0.0077\n",
            "\n",
            "Training model 108/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0103, MAE: 0.0781, MSE: 0.0103\n",
            "\n",
            "Training model 109/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0098, MAE: 0.0766, MSE: 0.0098\n",
            "\n",
            "Training model 110/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0085, MAE: 0.0712, MSE: 0.0085\n",
            "\n",
            "Training model 111/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0081, MAE: 0.0704, MSE: 0.0081\n",
            "\n",
            "Training model 112/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.2, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0080, MAE: 0.0701, MSE: 0.0080\n",
            "\n",
            "Training model 113/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0078, MAE: 0.0692, MSE: 0.0078\n",
            "\n",
            "Training model 114/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0659, MSE: 0.0071\n",
            "\n",
            "Training model 115/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0075, MAE: 0.0677, MSE: 0.0075\n",
            "\n",
            "Training model 116/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0660, MSE: 0.0071\n",
            "\n",
            "Training model 117/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0088, MAE: 0.0737, MSE: 0.0088\n",
            "\n",
            "Training model 118/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0078, MAE: 0.0685, MSE: 0.0078\n",
            "\n",
            "Training model 119/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0073, MAE: 0.0669, MSE: 0.0073\n",
            "\n",
            "Training model 120/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 1, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0071, MAE: 0.0660, MSE: 0.0071\n",
            "\n",
            "Training model 121/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0087, MAE: 0.0722, MSE: 0.0087\n",
            "\n",
            "Training model 122/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0077, MAE: 0.0685, MSE: 0.0077\n",
            "\n",
            "Training model 123/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0081, MAE: 0.0697, MSE: 0.0081\n",
            "\n",
            "Training model 124/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: adam, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0106, MAE: 0.0817, MSE: 0.0106\n",
            "\n",
            "Training model 125/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 20\n",
            "Test Loss: 0.0091, MAE: 0.0743, MSE: 0.0091\n",
            "\n",
            "Training model 126/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.001, Epochs: 30\n",
            "Test Loss: 0.0088, MAE: 0.0729, MSE: 0.0088\n",
            "\n",
            "Training model 127/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 20\n",
            "Test Loss: 0.0079, MAE: 0.0691, MSE: 0.0079\n",
            "\n",
            "Training model 128/128\n",
            "Activation: relu, Neurons: 100, Dropout: 0.3, Layers: 2, Optimizer: rmsprop, Learning Rate: 0.01, Epochs: 30\n",
            "Test Loss: 0.0090, MAE: 0.0735, MSE: 0.0090\n",
            "\n",
            "Top 5 configurations based on MSE:\n",
            "  Activation Function  Number of Neurons  Dropout Rate  Number of Layers  \\\n",
            "0                tanh                100           0.2                 2   \n",
            "1                tanh                100           0.2                 1   \n",
            "2                tanh                 50           0.2                 1   \n",
            "3                tanh                 50           0.2                 1   \n",
            "4                relu                100           0.2                 1   \n",
            "\n",
            "  Optimizer  Learning Rate  Epochs  Test Loss       MAE       MSE  \n",
            "0   rmsprop           0.01      20   0.007019  0.065992  0.007019  \n",
            "1      adam           0.01      30   0.007026  0.065989  0.007026  \n",
            "2   rmsprop           0.01      30   0.007039  0.065992  0.007039  \n",
            "3   rmsprop           0.01      20   0.007039  0.065971  0.007039  \n",
            "4   rmsprop           0.01      30   0.007046  0.065715  0.007046  \n",
            "\n",
            "Best hyperparameter configuration saved to 'best_hyperparameters.csv'.\n",
            "\n",
            "Training the best model for visualization...\n",
            "Epoch 1/20\n",
            "91/91 [==============================] - 7s 48ms/step - loss: 0.0829 - mae: 0.1766 - mse: 0.0829 - val_loss: 0.0136 - val_mae: 0.0932 - val_mse: 0.0136\n",
            "Epoch 2/20\n",
            "91/91 [==============================] - 3s 38ms/step - loss: 0.0207 - mae: 0.1137 - mse: 0.0207 - val_loss: 0.0097 - val_mae: 0.0780 - val_mse: 0.0097\n",
            "Epoch 3/20\n",
            "91/91 [==============================] - 4s 40ms/step - loss: 0.0138 - mae: 0.0922 - mse: 0.0138 - val_loss: 0.0109 - val_mae: 0.0838 - val_mse: 0.0109\n",
            "Epoch 4/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0125 - mae: 0.0879 - mse: 0.0125 - val_loss: 0.0088 - val_mae: 0.0750 - val_mse: 0.0088\n",
            "Epoch 5/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0116 - mae: 0.0843 - mse: 0.0116 - val_loss: 0.0096 - val_mae: 0.0780 - val_mse: 0.0096\n",
            "Epoch 6/20\n",
            "91/91 [==============================] - 4s 41ms/step - loss: 0.0113 - mae: 0.0832 - mse: 0.0113 - val_loss: 0.0119 - val_mae: 0.0886 - val_mse: 0.0119\n",
            "Epoch 7/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0109 - mae: 0.0823 - mse: 0.0109 - val_loss: 0.0080 - val_mae: 0.0714 - val_mse: 0.0080\n",
            "Epoch 8/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0107 - mae: 0.0808 - mse: 0.0107 - val_loss: 0.0090 - val_mae: 0.0760 - val_mse: 0.0090\n",
            "Epoch 9/20\n",
            "91/91 [==============================] - 4s 41ms/step - loss: 0.0105 - mae: 0.0804 - mse: 0.0105 - val_loss: 0.0117 - val_mae: 0.0879 - val_mse: 0.0117\n",
            "Epoch 10/20\n",
            "91/91 [==============================] - 4s 40ms/step - loss: 0.0104 - mae: 0.0798 - mse: 0.0104 - val_loss: 0.0076 - val_mae: 0.0694 - val_mse: 0.0076\n",
            "Epoch 11/20\n",
            "91/91 [==============================] - 3s 38ms/step - loss: 0.0105 - mae: 0.0803 - mse: 0.0105 - val_loss: 0.0084 - val_mae: 0.0726 - val_mse: 0.0084\n",
            "Epoch 12/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0099 - mae: 0.0778 - mse: 0.0099 - val_loss: 0.0099 - val_mae: 0.0793 - val_mse: 0.0099\n",
            "Epoch 13/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0100 - mae: 0.0788 - mse: 0.0100 - val_loss: 0.0131 - val_mae: 0.0928 - val_mse: 0.0131\n",
            "Epoch 14/20\n",
            "91/91 [==============================] - 4s 39ms/step - loss: 0.0101 - mae: 0.0786 - mse: 0.0101 - val_loss: 0.0076 - val_mae: 0.0689 - val_mse: 0.0076\n",
            "Epoch 15/20\n",
            "91/91 [==============================] - 3s 38ms/step - loss: 0.0099 - mae: 0.0782 - mse: 0.0099 - val_loss: 0.0077 - val_mae: 0.0690 - val_mse: 0.0077\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best LSTM model saved as 'best_lstm_model.h5'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# **Hyperparameter Optimization for LSTM on Daily Minimum Temperatures Dataset**\n",
        "\n",
        "# **Step 1: Import Necessary Libraries**\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import product\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "# **Step 2: Download the Dataset Directly from URL**\n",
        "\n",
        "# Define the dataset URL\n",
        "dataset_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "\n",
        "# Download the dataset using pandas\n",
        "data = pd.read_csv(dataset_url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# **Step 3: Load and Preprocess the Data**\n",
        "\n",
        "def load_and_preprocess_data(data, sequence_length=30, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset for LSTM modeling.\n",
        "\n",
        "    Parameters:\n",
        "        data (DataFrame): The raw dataset.\n",
        "        sequence_length (int): Number of past days to use for prediction.\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: Split and preprocessed data.\n",
        "        scaler: Fitted MinMaxScaler object.\n",
        "    \"\"\"\n",
        "    # Parse the 'Date' column to datetime\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data.set_index('Date', inplace=True)\n",
        "\n",
        "    # Check for missing values\n",
        "    if data.isnull().values.any():\n",
        "        print(\"Dataset contains missing values. Handling missing values...\")\n",
        "        data = data.dropna()\n",
        "\n",
        "    # Feature Scaling\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data[['Temp']])\n",
        "\n",
        "    # Create sequences\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(sequence_length, len(scaled_data)):\n",
        "        X.append(scaled_data[i-sequence_length:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape X for LSTM [samples, timesteps, features]\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTotal samples: {X.shape[0]}\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "# Preprocess the data\n",
        "sequence_length = 30  # Using past 30 days to predict the next day\n",
        "X_train, X_test, y_train, y_test, scaler = load_and_preprocess_data(data, sequence_length, test_size=0.2)\n",
        "\n",
        "# **Step 4: Define the LSTM Model Builder Function**\n",
        "\n",
        "def build_model(num_layers, num_neurons, dropout_rate, activation, optimizer, learning_rate, input_shape):\n",
        "    \"\"\"\n",
        "    Builds and compiles an LSTM model based on the provided hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "        num_layers (int): Number of LSTM layers.\n",
        "        num_neurons (int): Number of neurons in each LSTM layer.\n",
        "        dropout_rate (float): Dropout rate after each LSTM layer.\n",
        "        activation (str): Activation function for LSTM layers.\n",
        "        optimizer (str): Optimizer type ('adam', 'rmsprop', 'sgd').\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        input_shape (tuple): Shape of the input data (timesteps, features).\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        return_sequences = True if i < num_layers - 1 else False\n",
        "        if i == 0:\n",
        "            model.add(LSTM(units=num_neurons, activation=activation, return_sequences=return_sequences, input_shape=input_shape))\n",
        "        else:\n",
        "            model.add(LSTM(units=num_neurons, activation=activation, return_sequences=return_sequences))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "\n",
        "    # Configure the optimizer with the learning rate\n",
        "    if optimizer.lower() == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer.lower() == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer.lower() == 'sgd':\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer type\")\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# **Step 5: Define the Hyperparameter Grid**\n",
        "\n",
        "def get_hyperparameter_grid():\n",
        "    \"\"\"\n",
        "    Defines the hyperparameter grid for optimization.\n",
        "\n",
        "    Returns:\n",
        "        List of hyperparameter combinations.\n",
        "    \"\"\"\n",
        "    activation_functions = ['tanh', 'relu']\n",
        "    num_neurons_options = [50, 100]\n",
        "    dropout_rates = [0.2, 0.3]\n",
        "    num_layers_options = [1, 2]\n",
        "    optimizers = ['adam', 'rmsprop']\n",
        "    learning_rates = [0.001, 0.01]\n",
        "    epochs_options = [20, 30]\n",
        "\n",
        "    hyperparameter_combinations = list(product(\n",
        "        activation_functions,\n",
        "        num_neurons_options,\n",
        "        dropout_rates,\n",
        "        num_layers_options,\n",
        "        optimizers,\n",
        "        learning_rates,\n",
        "        epochs_options\n",
        "    ))\n",
        "\n",
        "    print(f\"\\nTotal hyperparameter combinations: {len(hyperparameter_combinations)}\")\n",
        "    return hyperparameter_combinations\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "hyperparameter_combinations = get_hyperparameter_grid()\n",
        "\n",
        "# **Step 6: Perform Hyperparameter Optimization**\n",
        "\n",
        "def hyperparameter_optimization(X_train, X_test, y_train, y_test, hyperparameter_combinations, csv_file):\n",
        "    \"\"\"\n",
        "    Iterates through hyperparameter combinations, trains and evaluates models, and records results.\n",
        "\n",
        "    Parameters:\n",
        "        X_train, X_test, y_train, y_test: Preprocessed data splits.\n",
        "        hyperparameter_combinations (list): List of hyperparameter tuples.\n",
        "        csv_file (str): Path to the CSV file to save results.\n",
        "    \"\"\"\n",
        "    # Prepare CSV file\n",
        "    header = [\n",
        "        'Activation Function', 'Number of Neurons', 'Dropout Rate',\n",
        "        'Number of Layers', 'Optimizer', 'Learning Rate', 'Epochs',\n",
        "        'Test Loss', 'MAE', 'MSE'\n",
        "    ]\n",
        "\n",
        "    # Write header if file does not exist\n",
        "    if not os.path.exists(csv_file):\n",
        "        with open(csv_file, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Iterate through hyperparameter combinations\n",
        "    for idx, (activation, neurons, dropout, layers, optimizer, lr, epochs) in enumerate(hyperparameter_combinations):\n",
        "        print(f\"\\nTraining model {idx+1}/{len(hyperparameter_combinations)}\")\n",
        "        print(f\"Activation: {activation}, Neurons: {neurons}, Dropout: {dropout}, Layers: {layers}, Optimizer: {optimizer}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
        "\n",
        "        # Build the model\n",
        "        model = build_model(layers, neurons, dropout, activation, optimizer, lr, input_shape)\n",
        "\n",
        "        # Early stopping to prevent overfitting\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0  # Change to 1 for more detailed output\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        evaluation = model.evaluate(X_test, y_test, verbose=0)\n",
        "        test_loss, mae, mse = evaluation\n",
        "\n",
        "        print(f\"Test Loss: {test_loss:.4f}, MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
        "\n",
        "        # Append results to CSV\n",
        "        with open(csv_file, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                activation, neurons, dropout, layers, optimizer, lr, epochs,\n",
        "                test_loss, mae, mse\n",
        "            ])\n",
        "\n",
        "# Define the path for the results CSV\n",
        "results_csv = 'hyperparameter_results.csv'\n",
        "\n",
        "# Perform hyperparameter optimization\n",
        "hyperparameter_optimization(X_train, X_test, y_train, y_test, hyperparameter_combinations, results_csv)\n",
        "\n",
        "# **Step 7: Identify and Save the Best Hyperparameters**\n",
        "\n",
        "def find_best_hyperparameters(csv_file, metric='MSE', ascending=True):\n",
        "    \"\"\"\n",
        "    Finds and saves the best hyperparameter configuration based on the specified metric.\n",
        "\n",
        "    Parameters:\n",
        "        csv_file (str): Path to the CSV file containing results.\n",
        "        metric (str): Metric to sort by ('Test Loss', 'MAE', 'MSE').\n",
        "        ascending (bool): Sort order.\n",
        "    \"\"\"\n",
        "    # Load the results\n",
        "    results = pd.read_csv(csv_file)\n",
        "\n",
        "    # Check if the metric exists\n",
        "    if metric not in results.columns:\n",
        "        print(f\"Metric '{metric}' not found in the results.\")\n",
        "        return\n",
        "\n",
        "    # Sort the results based on the specified metric\n",
        "    best_results = results.sort_values(by=metric, ascending=ascending).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nTop 5 configurations based on {metric}:\")\n",
        "    print(best_results.head(5))\n",
        "\n",
        "    # Save the best configuration\n",
        "    best_configuration = best_results.iloc[0]\n",
        "    best_configuration.to_frame().T.to_csv('best_hyperparameters.csv', index=False)\n",
        "    print(\"\\nBest hyperparameter configuration saved to 'best_hyperparameters.csv'.\")\n",
        "\n",
        "# Identify and save the best hyperparameters based on MSE\n",
        "find_best_hyperparameters(results_csv, metric='MSE', ascending=True)\n",
        "\n",
        "# **Step 8: Download the Results CSV Files**\n",
        "\n",
        "# To download the CSV files to your local machine, uncomment the following lines.\n",
        "# from google.colab import files\n",
        "# files.download('hyperparameter_results.csv')\n",
        "# files.download('best_hyperparameters.csv')\n",
        "\n",
        "# **Step 9: Visualize Training and Validation Loss for the Best Model**\n",
        "\n",
        "# Optional: Visualize the training history of the best model\n",
        "def visualize_training_history(model, X_train, y_train, X_test, y_test, epochs, batch_size=32):\n",
        "    \"\"\"\n",
        "    Trains the model and plots the training and validation loss.\n",
        "\n",
        "    Parameters:\n",
        "        model: Compiled Keras model.\n",
        "        X_train, y_train: Training data.\n",
        "        X_test, y_test: Testing data.\n",
        "        epochs (int): Number of epochs to train.\n",
        "        batch_size (int): Batch size for training.\n",
        "    \"\"\"\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_test, y_test),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "# Example: Visualize for the best model\n",
        "# Load the best hyperparameters\n",
        "best_config = pd.read_csv('best_hyperparameters.csv')\n",
        "\n",
        "# Extract hyperparameters\n",
        "best_activation = best_config['Activation Function'].values[0]\n",
        "best_neurons = best_config['Number of Neurons'].values[0]\n",
        "best_dropout = best_config['Dropout Rate'].values[0]\n",
        "best_layers = best_config['Number of Layers'].values[0]\n",
        "best_optimizer = best_config['Optimizer'].values[0]\n",
        "best_lr = best_config['Learning Rate'].values[0]\n",
        "best_epochs = int(best_config['Epochs'].values[0])\n",
        "\n",
        "print(\"\\nTraining the best model for visualization...\")\n",
        "\n",
        "# Build the best model\n",
        "best_model = build_model(best_layers, best_neurons, best_dropout, best_activation, best_optimizer, best_lr, (X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "# Early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the best model\n",
        "history = best_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=best_epochs,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Best Model Loss Over Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# **Step 10: Save the Best Model (Optional)**\n",
        "\n",
        "# Save the best model for future use\n",
        "best_model.save('best_lstm_model.h5')\n",
        "print(\"\\nBest LSTM model saved as 'best_lstm_model.h5'.\")\n",
        "\n",
        "# To download the model, uncomment the following lines.\n",
        "# from google.colab import files\n",
        "# files.download('best_lstm_model.h5')"
      ]
    }
  ]
}